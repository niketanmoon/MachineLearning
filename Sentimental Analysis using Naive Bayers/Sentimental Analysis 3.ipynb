{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888c5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.read_csv(\"imdb_master.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a23ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset.iloc[:, 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640fcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset[full_dataset.label != \"unsup\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32623af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punctuation(review):\n",
    "    return re.sub(r'[^\\w\\s]', \"\", review)\n",
    "\n",
    "def convert_to_lowercase(review):\n",
    "    return review.lower()\n",
    "\n",
    "def remove_numbers(review):\n",
    "    return re.sub(r'[\\d+]', \"\", review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75606c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[\"review\"] = full_dataset.review.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab820dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[\"review\"] = full_dataset.review.apply(convert_to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[\"review\"] = full_dataset.review.apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950dcde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff91a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words(\"english\")\n",
    "def apply_tokenization_and_remove_stopwords(review):\n",
    "    # Applying tokenization\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    # applying removal of stopwords\n",
    "    review_no_stopwords = [word for word in tokens if word not in stopwords_list]\n",
    "    return \" \".join(review_no_stopwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[\"review\"] = full_dataset.review.apply(apply_tokenization_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "def apply_lemmatization(review):\n",
    "    lemmatized_review = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    for w in tokens:\n",
    "        lemmatized_review.append(lemmatizer.lemmatize(w))\n",
    "    return \" \".join(lemmatized_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dd90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset[\"review\"] = full_dataset.review.apply(apply_lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db22dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiNB:\n",
    "    def __init__(self,alpha=1):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def _prior(self): # CHECKED\n",
    "        \"\"\"\n",
    "        Calculates prior for each unique class in y. P(y)\n",
    "        \"\"\"\n",
    "        P = np.zeros((self.n_classes_))\n",
    "        _, self.dist = np.unique(self.y,return_counts=True)\n",
    "        for i in range(self.classes_.shape[0]):\n",
    "            P[i] = self.dist[i] / self.n_samples\n",
    "        return P\n",
    "            \n",
    "    def fit(self, X, y): # CHECKED, matches with sklearn\n",
    "        \"\"\"\n",
    "        Calculates the following things- \n",
    "            class_priors_ is list of priors for each y.\n",
    "            N_yi: 2D array. Contains for each class in y, the number of time each feature i appears under y.\n",
    "            N_y: 1D array. Contains for each class in y, the number of all features appear under y.\n",
    "            \n",
    "        params\n",
    "        ------\n",
    "        X: 2D array. shape(n_samples, n_features)\n",
    "            Multinomial data\n",
    "        y: 1D array. shape(n_samples,). Labels must be encoded to integers.\n",
    "        \"\"\"\n",
    "        self.y = y\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes_ = self.classes_.shape[0]\n",
    "        self.class_priors_ = self._prior()\n",
    "        \n",
    "        # distinct values in each features\n",
    "        self.uniques = []\n",
    "        for i in range(self.n_features):\n",
    "            tmp = np.unique(X[:,i])\n",
    "            self.uniques.append( tmp )\n",
    "            \n",
    "        self.N_yi = np.zeros((self.n_classes_, self.n_features)) # feature count\n",
    "        self.N_y = np.zeros((self.n_classes_)) # total count \n",
    "        for i in self.classes_: # x axis\n",
    "            indices = np.argwhere(self.y==i).flatten()\n",
    "            columnwise_sum = []\n",
    "            for j in range(self.n_features): # y axis\n",
    "                columnwise_sum.append(np.sum(X[indices,j]))\n",
    "                \n",
    "            self.N_yi[i] = columnwise_sum # 2d\n",
    "            self.N_y[i] = np.sum(columnwise_sum) # 1d\n",
    "            \n",
    "    def _theta(self, x_i, i, h):\n",
    "        \"\"\"\n",
    "        Calculates theta_yi. aka P(xi | y) using eqn(1) in the notebook.\n",
    "        \n",
    "        params\n",
    "        ------\n",
    "        x_i: int. \n",
    "            feature x_i\n",
    "            \n",
    "        i: int.\n",
    "            feature index. \n",
    "            \n",
    "        h: int or string.\n",
    "            a class in y\n",
    "        \n",
    "        returns\n",
    "        -------\n",
    "        theta_yi: P(xi | y)\n",
    "        \"\"\"\n",
    "        \n",
    "        Nyi = self.N_yi[h,i]\n",
    "        Ny  = self.N_y[h]\n",
    "        \n",
    "        numerator = Nyi + self.alpha\n",
    "        denominator = Ny + (self.alpha * self.n_features)\n",
    "        \n",
    "        return  (numerator / denominator)**x_i\n",
    "    \n",
    "    def _likelyhood(self, x, h):\n",
    "        \"\"\"\n",
    "        Calculates P(E|H) = P(E1|H) * P(E2|H) .. * P(En|H).\n",
    "        \n",
    "        params\n",
    "        ------\n",
    "        x: array. shape(n_features,)\n",
    "            a row of data.\n",
    "        h: int. \n",
    "            a class in y\n",
    "        \"\"\"\n",
    "        tmp = []\n",
    "        for i in range(x.shape[0]):\n",
    "            tmp.append(self._theta(x[i], i,h))\n",
    "        \n",
    "        return np.prod(tmp)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        samples, features = X.shape\n",
    "        self.predict_proba = np.zeros((samples,self.n_classes_))\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            joint_likelyhood = np.zeros((self.n_classes_))\n",
    "            \n",
    "            for h in range(self.n_classes_):\n",
    "                joint_likelyhood[h]  = self.class_priors_[h] * self._likelyhood(X[i],h) # P(y) P(X|y) \n",
    "                \n",
    "            denominator = np.sum(joint_likelyhood)\n",
    "            \n",
    "            for h in range(self.n_classes_):\n",
    "                numerator = joint_likelyhood[h]\n",
    "                self.predict_proba[i,h] = (numerator / denominator)\n",
    "            \n",
    "        indices = np.argmax(self.predict_proba,axis=1)\n",
    "        return self.classes_[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd769b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = full_dataset[full_dataset[\"type\"] == \"train\"]\n",
    "test_dataset = full_dataset[full_dataset[\"type\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ffb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7152799",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(train_dataset['review']).toarray()\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(train_dataset['label']).ravel()\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "X_test = cv.fit_transform(test_dataset['review']).toarray()\n",
    "y_test = lb.fit_transform(test_dataset['label']).ravel()\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99822977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42db994",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "me = MultiNB()\n",
    "me.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44456559",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yhat = me.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c15ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_val, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1fdb0a",
   "metadata": {},
   "source": [
    "# Now going for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_pred = me.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def find_f1_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    true_positive_and_false_negative = cm.sum(1)\n",
    "    true_positive_and_false_positive = cm.sum(0)\n",
    "    true_positive = cm.diagonal()\n",
    "    precision = true_positive / true_positive_and_false_positive\n",
    "    recall = true_positive / true_positive_and_false_negative\n",
    "\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "print(find_f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
